# -*- coding: utf-8 -*-
"""MLP-Lotes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1glyxcanNCqbSZGwmHOScZCUdu18SZ_VU
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from  matplotlib import pyplot as plt
# %matplotlib inline
plt.rcParams['figure.figsize'] = (10, 5)
plt.style.use('ggplot')
plt.rcParams["legend.loc"]
import math
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import random
random.seed(123)

class MLP(object):
  
  def __init__(self,num_entradas=3, num_caoculta=[3,3], num_salida=2, activacion='tanh',metrica='MSE', optimizador='SGD', perdida='RMSE'):
    self.num_entradas=num_entradas
    self.num_caoculta=num_caoculta
    self.num_salida=num_salida
    self.metrica=metrica
    self.perdida=perdida

    def tanh(x):return np.tanh(x)
    def d_tanh(x): return 1 - np.square(np.tanh(x))

    def sigmoid(x): return 1/(1 + np.exp(-x))
    def d_sigmoid(x): return (1 - sigmoid(x))*sigmoid(x)

    def softplus(x): return np.log(1+np.exp(x))
    def logisticF(x): return np.exp(x)/(1+np.exp(x)) #Derivada de la SoftPlus
    def d_logisticF(x): return np.exp(x)/((1+np.exp(x))**2)

    def gauss(x, sigma=2 , c=0): return np.exp(-(x-c)**2/(2*sigma**2))
    def d_gauss(x, A=1, B=1): return -2*np.exp(-B*(x)**2)*A*B*x

    def sen(x,A=1,O=1,B=0): return A*np.sin(O*x+B)
    def d_sen(x,A=1,O=1,B=1): return A*np.cos(O*x+B)*O
    
    def relu(x): return max(0,x)
    def d_relu(x): 
      if x>=0:
         return 1
      else:
        return 0
    def prelu(x,p): return max(0,x)+p*min(0,x)
    def d_prelu(x):
      if x>0:
        return 0
      else:
        return x
    FuncionesActivacion = {
        'tanh': (tanh, d_tanh),
        'sigmoid': (sigmoid, d_sigmoid),
        'sofplus' : (softplus,logisticF),
        'logistic': (logisticF,d_logisticF),
        'gauss':(gauss,d_gauss),
        'seno':(sen,d_sen),
        'relu':(relu,d_relu),
        'prelu':(prelu,d_prelu)
    }
    self.FuncionesActivacion =FuncionesActivacion 
    self.act, self.d_act = self.FuncionesActivacion.get(activacion)

    def propagacion_atras_SGD(error, tasa_aprendizaje,eps, beta1, beta2, alp):
      for i in reversed(range(len(self.dW))):
        activaciones=self.activaciones[i+1]
        delta = np.multiply(self.d_act(activaciones),error)
        activaciones_actuales=self.activaciones[i]
        activaciones_actuales = activaciones_actuales.reshape(activaciones_actuales.shape[0],-1)
        self.dW[i] = 1/delta.shape[0]*np.dot(activaciones_actuales,delta.T)
        self.db[i] = 1/delta.shape[0]*np.sum(delta, axis=1, keepdims=True)
        error = np.dot(self.pesos[i], delta)

        self.pesos[i]=self.pesos[i]+self.dW[i]*tasa_aprendizaje
        self.bias[i]=self.bias[i]+self.db[i]*tasa_aprendizaje
      
        return error
    
    def propagacion_atras_Momentum(error, tasa_aprendizaje,eps, beta1, beta2, alp):
      for i in reversed(range(len(self.dW))):
        activaciones=self.activaciones[i+1]
        delta = np.multiply(self.d_act(activaciones),error)
        activaciones_actuales=self.activaciones[i]
        activaciones_actuales = activaciones_actuales.reshape(activaciones_actuales.shape[0],-1)
        self.dW[i] = 1/delta.shape[0]*np.dot(activaciones_actuales,delta.T)
        self.db[i] = 1/delta.shape[0]*np.sum(delta, axis=1, keepdims=True)
        error = np.dot(self.pesos[i], delta)
        
        self.cacheW[i]=alp*self.cacheW[i]+tasa_aprendizaje*dW[i]
        self.cacheB[i]=alp*self.cacheB[i]+tasa_aprendizaje*db[i]

        self.pesos[i]=self.pesos[i]+self.cacheW[i]
        self.bias[i]=self.bias[i]+self.cacheB[i]

        return error
        
    def propagacion_atras_Adagrad(error, tasa_aprendizaje,eps, beta1, beta2, alp):
      for i in reversed(range(len(self.dW))):
        activaciones=self.activaciones[i+1]
        delta = np.multiply(self.d_act(activaciones),error)
        activaciones_actuales=self.activaciones[i]
        activaciones_actuales = activaciones_actuales.reshape(activaciones_actuales.shape[0],-1)
        self.dW[i] = 1/delta.shape[0]*np.dot(activaciones_actuales,delta.T)
        self.db[i] = 1/delta.shape[0]*np.sum(delta, axis=1, keepdims=True)
        error = np.dot(self.pesos[i], delta)

        self.cacheW[i]+=self.dW[i]**2
        self.cacheB[i]+=self.db[i]**2

        self.pesos[i]=self.pesos[i]+tasa_aprendizaje*(self.dW[i]/(np.sqrt(self.cacheW[i])+eps))
        self.bias[i]=self.bias[i]+tasa_aprendizaje*(self.db[i]/(np.sqrt(self.cacheB[i])+eps))
      
        return error

    def propagacion_atras_Adam(error, tasa_aprendizaje,eps, beta1, beta2, alp):
      for i in reversed(range(len(self.dW))):
        activaciones=self.activaciones[i+1]
        delta = np.multiply(self.d_act(activaciones),error)
        activaciones_actuales=self.activaciones[i]
        activaciones_actuales = activaciones_actuales.reshape(activaciones_actuales.shape[0],-1)
        self.dW[i] = 1/delta.shape[0]*np.dot(activaciones_actuales,delta.T)
        self.db[i] = 1/delta.shape[0]*np.sum(delta, axis=1, keepdims=True)
        error = np.dot(self.pesos[i], delta)

        self.mW[i]=beta1*self.mW[i]+(1-beta1)*dW[i]
        self.mtW[i]=self.mW[i]/(1-beta1)
        self.vW[i]=beta2*self.vW[i]+(1-beta2)*(dW[i]**2)
        self.vtW[i]=self.vW[i]/(1-beta2)

        self.mB[i]=beta1*self.mB[i]+(1-beta1)*db[i]
        self.mtB[i]=self.mB[i]/(1-beta1)
        self.vB[i]=beta2*self.vB[i]+(1-beta2)*(db[i]**2)
        self.vtB[i]=self.vB[i]/(1-beta2)

        self.pesos[i]=self.pesos[i]+(tasa_aprendizaje/(np.sqrt(self.vtW[i])+eps))*self.mtW[i]
        self.bias[i]=self.bias[i]+(tasa_aprendizaje/(np.sqrt(self.vtB[i])+eps))*self.mtB[i]

        return error

    Optimizador= {
        'SGD': propagacion_atras_SGD,
        'Adagrad': propagacion_atras_Adagrad,
        'Momentum': propagacion_atras_Momentum,
        'Adam': propagacion_atras_Adam
    }

    self.Optimizador=Optimizador
    self.propagacion_atras=self.Optimizador.get(optimizador)
    def MSE(y_hat, y_verdadero): return np.average((y_hat-y_verdadero)**2)
    def l_MSE(y,y_hat): return np.median(1/2*np.average((y-y_hat)**2))

    def RMSE(y_hat, y_verdadero): return np.sqrt(np.average((y_hat-y_verdadero)**2))
    def l_RMSE(y,y_hat): return np.sqrt(np.median(1/2*np.average((y-y_hat)**2)))

    Metricas = {
        'MSE': (MSE),
        'RMSE': (RMSE)
    }
    Perdidas = {
        'MSE': (l_MSE),
        'RMSE': (l_RMSE)
    }
    self.Metricas = Metricas
    self.met = self.Metricas.get(metrica)
    self.Perdidas = Perdidas
    self.l_met = self.Metricas.get(perdida)

    capas=[num_entradas]+num_caoculta+[num_salida]

    bias=[]
    pesos=[]
    for i in range(len(capas)-1):
      w=np.random.rand(capas[i], capas[i+1])
      b=np.random.randn(capas[i+1]).reshape(1, capas[i+1])
      pesos.append(w)
      bias.append(b)
    self.pesos=pesos
    self.bias=bias

    activaciones=[]
    for i in range(len(capas)):
      act=np.zeros(capas[i])
      activaciones.append(act)  
    self.activaciones=activaciones

    dW=[]
    db=[]
    for i in range(len(capas)-1):
      derW=np.zeros((capas[i], capas[i+1]))
      derb=np.zeros((capas[i+1])).reshape(1, capas[i+1])
      dW.append(derW)  
      db.append(derb)
    self.dW=dW  
    self.db=db
    

    cacheW=[]
    cacheB=[]
    for i in range(len(capas)-1):
      Cachew=np.zeros((capas[i], capas[i+1]))
      Cacheb=np.zeros((capas[i+1])).reshape(1, capas[i+1])
      cacheW.append(Cachew)  
      cacheB.append(Cacheb)  

    self.cacheW=cacheW
    self.cacheB=cacheB

    mW=[]
    mtW=[]
    vW=[]
    vtW=[]
    mB=[]
    mtB=[]
    vB=[]
    vtB=[]
    for i in range(len(capas)-1):
      mw=np.zeros((capas[i], capas[i+1]))
      mtw=np.zeros((capas[i], capas[i+1]))
      vw=np.zeros((capas[i], capas[i+1]))
      vtw=np.zeros((capas[i], capas[i+1]))

      mb=np.zeros((capas[i+1])).reshape(1, capas[i+1])
      mtb=np.zeros((capas[i+1])).reshape(1, capas[i+1])
      vb=np.zeros((capas[i+1])).reshape(1, capas[i+1])
      vtb=np.zeros((capas[i+1])).reshape(1, capas[i+1])

      mW.append(mw) 
      mtW.append(mtw)  
      vW.append(vw) 
      vtW.append(vtw) 
      mB.append(mb) 
      mtB.append(mtb) 
      vB.append(vb)  
      vtB.append(vtb)   
       
    self.mW=mW
    self.mtW=mtW
    self.vW=vW
    self.vtW=vtW
    self.mB=mB
    self.mtB=mtB
    self.vB=vB
    self.vtB=vtB

  def propagacion_adelante(self, entradas):
    activaciones=entradas
    self.activaciones[0]=entradas 
    for i,w in enumerate(self.pesos):
      activaciones=self.act(np.matmul(activaciones,w)+self.bias[i])
      self.activaciones[i+1]=activaciones.T
    return activaciones


  def derivas(self,y,y_hat):
    if self.perdida=='MSE': error=(2*(y-y_hat))/len(y_hat)  
    elif self.perdida=='RMSE': error=1/(2*np.sqrt((2*abs((y-y_hat)))/len(y_hat))) 
    return error 
  
  def entrenamiento(self,X_train, y_train,x_val, y_val, epocas,batch_size, tasa_aprendizaje=0.1,eps=1e-8,beta1=0.9,beta2=0.999, alp=0.9):
    N = X_train.shape[0]
    n_batches = int(np.floor(N/batch_size))
    train_losses = []
    train_accs = []
    val_losses = []
    val_accs = []

    for epoch in range(epocas):
      Btrain_loss = []
      Btrain_accuracy = []
      Bval_loss = []
      Bval_accuracy = []
      l = 0
      acc = 0
      temp = 0

      for j, entradas in enumerate(X_train):
        entradas_val = y_train[j]
        for batch in range(n_batches):
          x = np.squeeze(X_train[batch*batch_size:batch_size+batch*batch_size]).T
          y = y_train[batch*batch_size:batch_size+batch*batch_size]
          y_hat= self.propagacion_adelante(entradas)
          error=self.derivas(entradas_val,y_hat)
          self.propagacion_atras(error,tasa_aprendizaje,eps, beta1, beta2, alp)
          l += self.l_met(y_train[batch*batch_size:batch_size+batch*batch_size],y_hat)
          acc += self.met(y_hat, y_train[batch*batch_size:batch_size+batch*batch_size])

        if N%batch_size != 0:
          x = np.squeeze(X_train[-1*(N%batch_size):]).T
          y = y_train[-1*(N%batch_size):]
          y_hat= self.propagacion_adelante(entradas)
          error=self.derivas(entradas_val,y_hat)
          self.propagacion_atras(error,tasa_aprendizaje,eps, beta1, beta2, alp)
          l += self.l_met(y_train[-1*(N%batch_size):],y_hat)
          acc += self.met(y_hat, y_train[-1*(N%batch_size):])
          temp = 1

        l = l/(n_batches + (N%batch_size))
        acc = acc/(n_batches + temp)

        Btrain_loss.append(l)
        Btrain_accuracy.append(acc)

      if x_val.any():
        y_val_hat= self.propagacion_adelante(x_val)
        val_acc = self.met(y_val_hat,y_val)
        val_l = self.l_met(y_val, y_val_hat)
        Bval_accuracy.append(val_acc)
        Bval_loss.append(val_l)

      train_loss, train_acc = np.mean(Btrain_loss), np.mean(Btrain_accuracy)
      val_loss, val_acc = np.mean(Bval_loss), np.mean(Bval_accuracy)
      
      train_losses.append(train_loss)
      train_accs.append(train_acc)
      val_losses.append(val_loss)
      val_accs.append(val_acc)
      print("Epoch: {}, Train Loss: {}, Train_acc: {}, val_loss: {}, val_Accuracy: {}".format(epoch+1,train_loss,train_acc,val_loss,val_acc))

      historico=pd.DataFrame({'Perdida Entrenamiento': train_losses, 'Perdida Validación': val_losses, 'Metrica Entrenamiento': train_accs, 'Metrica Validación': val_accs})
    return historico

  def seriesup(self,datos, numen=1, numsal=1, a=-1, b=1, dropnan=True):
    n_vars = 1 if type(datos) is list else datos.shape[1]
    df = pd.DataFrame(datos)
    cols, nombres = list(), list()
    for i in range(numen, 0, -1):
      cols.append(df.shift(i))
      nombres += [('variable%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
    for i in range(0, numsal):
      cols.append(df.shift(-i))
      if i == 0:
        nombres += [('variable%d(t)' % (j+1)) for j in range(n_vars)]
      else:
        nombres += [('variable%d(t+%d)' % (j+1, i)) for j in range(n_vars)]

      agg = pd.concat(cols, axis=1)
      agg.columns = nombres
      if dropnan:
          agg.dropna(inplace=True)
      return agg

  def validatest(self,datos,porcentaje=70):
    global x_val, y_val, x_entrenamiento,y_entrenamiento
    Da_to=df.shape[0] #len(df)
    Da_entre=round((Da_to*porcentaje)/100)
    valores = datos.values
    entrenamiento = valores[:Da_entre, :]
    test = valores[Da_entre:, :]
    x_entrenamiento, y_entrenamiento = entrenamiento[:, :-1], entrenamiento[:, -1]
    x_val, y_val = test[:, :-1], test[:, -1]
    print(x_entrenamiento.shape, y_entrenamiento.shape, x_val.shape, y_val.shape)

df = pd.read_csv('/content/drive/MyDrive/Tesis/Base de datos/DCOILWTICO2015.csv')
df.drop(df[df['DCOILWTICO'] == '.'].index, inplace = True)
df.dropna(inplace=True)
df['DATE']=pd.to_datetime(df['DATE'], format='%Y-%m-%d')
df = df.set_index('DATE')
df['DCOILWTICO']=df['DCOILWTICO'].astype(float)

mlp = MLP(8, [6,4,2], 1, activacion='tanh', metrica='RMSE', optimizador='Adam', perdida='MSE')

valores = df.values
valores = valores.astype('float32')
escalar = MinMaxScaler(feature_range=(-1, 1))
valores=valores.reshape(-1, 1) 
escalado = escalar.fit_transform(valores)

datos=mlp.seriesup(escalado,8,1)
data=mlp.validatest(datos,80)
x_entre=x_entrenamiento
y_entre=y_entrenamiento
x_vali=x_val
y_vali=y_val

epocas=62
lotes=8
hist=mlp.entrenamiento(x_entre,y_entre,x_vali,y_vali,epocas,lotes)

results = mlp.propagacion_adelante(x_val)
compara = pd.DataFrame(np.array([y_val, [x[0] for x in results]])).transpose()
compara.columns = ['real', 'prediccion']

inverted = escalar.inverse_transform(compara.values)

compara2 = pd.DataFrame(inverted)
compara2.columns = ['real', 'prediccion']

compara2 = pd.DataFrame(inverted)
compara2.columns = ['real', 'prediccion']

compara2['real'].plot(color="#031634",label="Valor Real WTI")
compara2['prediccion'].plot(color="#03565E",label="Predicción WTI")
plt.title('Precio del Petróleo crudo West Texas Intermediate')
plt.xlabel('Día')
plt.ylabel('WTI ($/bbl)')
plt.legend()
plt.xlim(0,len(results))
plt.show()

n=len(results)
RMSE=np.zeros(n)
RMSEsum=0

for i in range(0,len(results)):
  RMSE[i] = np.sqrt((compara2['real'][i] - compara2['prediccion'][i])**2)
  RMSEsum = np.sum(RMSE)/n
plt.plot(range(len(results)),RMSE, color="#031634")
plt.title('Raíz del Error Cuadrático Medio para los datos sin normalizar')
plt.xlabel('Mes')
plt.ylabel('RMSE')
plt.xlim(0,len(results))
plt.show()

print('El RMSE es de %0.3f' % RMSEsum)

RMSEData=pd.DataFrame(RMSE)
History=pd.DataFrame(hist)

fig, host = plt.subplots() 
    
par1 = host.twinx()
host.set_xlim(0, epocas-1)
    
host.set_xlabel("Epoca")
host.set_ylabel('Pérdida Conjunto de Entrenamiento')
par1.set_ylabel('Pérdida Conjunto de Validación')

p1, = host.plot(hist['Perdida Entrenamiento'], color="#031634",label="Función de Pérdida C.Entrenamiento")
p2, = par1.plot(hist['Perdida Validación'], color="#03565E" ,label="Función de Pérdida C.Validación")

lns = [p1, p2]
host.legend(handles=lns, loc='best')

plt.title('Función de Pérdida entre los conjuntos de Entrenamiento y Validación')
fig.tight_layout()
plt.savefig("Perdida-Epocas-MLP-Violky-ConLotes.jpg")

fig, host = plt.subplots() 
    
par1 = host.twinx()
host.set_xlim(0, epocas-1)

host.set_xlabel("Epoca")
host.set_ylabel('RMSE Conjunto de Entrenamiento')
par1.set_ylabel('RMSE Conjunto de Validación')

p1, = host.plot(hist['Metrica Entrenamiento'], color="#031634", label="RMSE C.Entrenamiento")
p2, = par1.plot(hist['Metrica Validación'], color="#03565E", label="RMSE C.Validación")

lns = [p1, p2]
host.legend(handles=lns, loc='best')
plt.title('Raíz del Error Cuadrático Medio entre los conjuntos de Entrenamiento y Validación')
fig.tight_layout()

vali = pd.read_csv('/content/drive/MyDrive/Tesis/Base de datos/Datos Validacion.csv')
vali.drop(vali[vali['DCOILWTICO'] == '.'].index, inplace = True)
vali.dropna(inplace=True)
vali['DATE']=pd.to_datetime(vali['DATE'], format='%Y-%m-%d')
vali = vali.set_index('DATE')
vali['DCOILWTICO']=vali['DCOILWTICO'].astype(float)

ultimosMeses = df['2022-01-01':'2022-07-01']
vaalores = ultimosMeses.values
valores = valores.astype('float32')
valores=valores.reshape(-1, 1)
scaled = escalar.fit_transform(valores)
reframed = mlp.seriesup(scaled, 8, 1)
reframed.drop(reframed.columns[[6]], axis=1, inplace=True)
reframed.head(6)

valores = reframed.values
x_test = vaalores[5:, :]
x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))
def agregarNuevoValor(x_test,nuevoValor):
  for i in range(x_test.shape[2]-1):
    x_test[0][0][i] = x_test[0][0][i+1]
    x_test[0][0][x_test.shape[2]-1]=nuevoValor
  return x_test

resul=[]
for i in range(31): 
  parcial=mlp.propagacion_adelante(x_test)
  resul.append(parcial[0])
  x_test=agregarNuevoValor(x_test,parcial[0])

adimen = [x for x in resul]    
adimen1=np.reshape(adimen, (31,1))
inverted = escalar.inverse_transform(adimen1)

prediccionAgosto = pd.DataFrame(inverted)
prediccionAgosto.columns = ['pronostico']
prediccionAgosto.plot()
prediccionAgosto.to_csv('pronostico-MLP-WTI-Violky-ConLotes.csv')